"""AI Service ‚Äî —Ç–æ–ª—å–∫–æ DeepSeek (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API). TTS ‚Äî —á–µ—Ä–µ–∑ Google –∏–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω–æ."""
import asyncio
import os
from openai import AsyncOpenAI
from typing import Optional, List, Dict, Any
import io
import json
import re
from app.config import settings
from app.schemas.session import GPTContextRequest, GPTResponse


class AIService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è DeepSeek API (—á–∞—Ç-–º–æ–¥–µ–ª–∏). TTS –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ‚Äî —Ç–æ–ª—å–∫–æ Google –≤ core."""

    def __init__(self):
        self.provider = "deepseek"
        api_key = (
            (settings.deepseek_api_key or os.environ.get("DEEPSEEK_API_KEY") or "")
            .strip()
        )
        if not api_key:
            raise ValueError(
                "DEEPSEEK_API_KEY –ø—É—Å—Ç–æ–π. –î–æ–±–∞–≤—å—Ç–µ –≤ backend/.env: DEEPSEEK_API_KEY=sk-..."
            )
        base_url = "https://api.deepseek.com"
        self.model_gpt = "deepseek-chat"
        print(f"[AI Service] Using DeepSeek API with model: {self.model_gpt}")

        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=120.0
        )
    
    async def generate_chat_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = 1000
    ) -> str:
        """
        Generate chat response using GPT-3.5
        
        Args:
            messages: List of messages with role and content
            system_prompt: System prompt for conversation context
            temperature: Sampling temperature (0-2)
            max_tokens: Maximum tokens in response
            
        Returns:
            Generated text response
        """
        try:
            # Prepare messages
            chat_messages = []
            
            if system_prompt:
                chat_messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            chat_messages.extend(messages)
            
            # Call GPT API
            response = await self.client.chat.completions.create(
                model=self.model_gpt,
                messages=chat_messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            raise Exception(f"Error generating chat response: {str(e)}")
    
    async def generate_interview_question(
        self,
        session_params: Dict[str, Any],
        conversation_history: List[Dict[str, str]],
        question_index: int
    ) -> str:
        """
        Generate interview question based on session parameters
        
        Args:
            session_params: Session configuration
            conversation_history: Previous messages in conversation
            question_index: Index of current question
            
        Returns:
            Generated question text
        """
        # Build system prompt for interview
        system_prompt = self._build_interview_system_prompt(session_params)
        
        # Get specific question if available
        questions = session_params.get("questions", [])
        if questions and question_index < len(questions):
            # Use predefined question, but GPT can rephrase if needed
            specific_question = questions[question_index]
            
            prompt = f"""You are an AI interviewer conducting an interview for the position: {session_params.get('position', 'N/A')}.

The next question to ask is: "{specific_question}"

Ask this question naturally in a {session_params.get('personality', 'professional')} manner. 
If the candidate has already answered this question partially, ask a follow-up or move to the next topic.
Keep the question concise and clear."""
        else:
            # Generate question based on context
            prompt = f"""You are an AI interviewer conducting an interview for the position: {session_params.get('position', 'N/A')}.

Generate the next question (question #{question_index + 1}) for the interview based on:
- Position: {session_params.get('position', 'N/A')}
- Evaluation criteria: {session_params.get('evaluation_criteria', [])}
- Interview type: {session_params.get('interview_type', 'screening')}

Ask a relevant question in a {session_params.get('personality', 'professional')} manner. 
Keep it concise and focused on assessing the candidate's suitability for the role."""

        messages = conversation_history + [
            {"role": "user", "content": prompt}
        ]
        
        return await self.generate_chat_response(
            messages=messages,
            system_prompt=system_prompt,
            temperature=0.8,
            max_tokens=200
        )
    
    async def generate_interview_response(
        self,
        user_message: str,
        session_params: Dict[str, Any],
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """
        Generate AI response during interview conversation
        
        Args:
            user_message: Candidate's message
            session_params: Session configuration
            conversation_history: Previous conversation
            question_index: Current question index
            
        Returns:
            AI response text
        """
        system_prompt = self._build_interview_system_prompt(session_params)
        
        messages = conversation_history + [
            {"role": "user", "content": user_message}
        ]
        
        return await self.generate_chat_response(
            messages=messages,
            system_prompt=system_prompt,
            temperature=0.7,
            max_tokens=300
        )
    
    async def text_to_speech(
        self,
        text: str,
        voice: Optional[str] = None,
        language: Optional[str] = "ru"
    ) -> bytes:
        """OpenAI TTS —É–±—Ä–∞–Ω –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ tts_service=google –≤ –∫–æ–Ω—Ñ–∏–≥–µ."""
        raise RuntimeError("OpenAI TTS —É–¥–∞–ª—ë–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ tts_service=google –≤ .env")
    
    def _build_interview_system_prompt(self, session_params: Dict[str, Any]) -> str:
        """Build system prompt for interview based on session parameters"""
        position = session_params.get("position", "the position")
        company = session_params.get("company", "")
        personality = session_params.get("personality", "professional")
        interview_type = session_params.get("interview_type", "screening")
        evaluation_criteria = session_params.get("evaluation_criteria", [])
        
        personality_descriptions = {
            "friendly": "friendly, warm, and encouraging",
            "professional": "professional, courteous, and structured",
            "motivating": "motivating, enthusiastic, and supportive"
        }
        
        personality_desc = personality_descriptions.get(personality, "professional")
        
        prompt = f"""You are an AI interviewer conducting a {interview_type} interview for the position: {position}"""
        
        if company:
            prompt += f" at {company}"
        
        prompt += f"""

Your role:
- Conduct the interview in a {personality_desc} manner
- Ask relevant questions based on the position requirements
- Listen actively and ask follow-up questions when needed
- Keep responses concise and professional
- Guide the conversation naturally
"""
        
        if evaluation_criteria:
            prompt += f"\nFocus on evaluating: {', '.join(evaluation_criteria[:5])}"
        
        prompt += """
- If you need clarification, ask: "–ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã —É—Ç–æ—á–Ω–∏—Ç—å?" or "Could you clarify?"
- If you didn't hear clearly, ask: "–ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å?" or "Could you repeat that?"
- When moving to next question, use natural transitions
"""
        
        return prompt
    
    async def analyze_transcript(
        self,
        transcript: List[Dict[str, Any]],
        session_params: Dict[str, Any],
        evaluation_criteria: List[str]
    ) -> Dict[str, Any]:
        """
        Analyze interview transcript using GPT-3.5 for evaluation
        
        Args:
            transcript: List of transcript messages
            session_params: Session configuration
            evaluation_criteria: Criteria for evaluation
            
        Returns:
            Analysis results with observations, strengths, improvements
        """
        # Build transcript text
        transcript_text = "\n".join([
            f"{msg.get('role', 'unknown')}: {msg.get('message', '')}"
            for msg in transcript
        ])
        
        system_prompt = f"""You are an expert HR analyst evaluating a job interview transcript.

Position: {session_params.get('position', 'N/A')}
Evaluation Criteria: {', '.join(evaluation_criteria)}

Analyze the candidate's responses and provide a comprehensive evaluation."""
        
        user_prompt = f"""Analyze this interview transcript and provide:

1. Overall assessment score (0-100)
2. Summary of the interview
3. Observations by category (stressHandling, empathy, problemSolving, conflictResolution, communication)
4. Strengths (at least 3)
5. Areas for improvement (at least 2)
6. Key effective phrases used by candidate
7. Key phrases that could be improved
8. Recommendation on readiness to work

Format your response as JSON with the following structure:
{{
    "score": <number 0-100>,
    "summary": "<text>",
    "readiness": "<text>",
    "observations": {{
        "stressHandling": "<observation>",
        "empathy": "<observation>",
        "problemSolving": "<observation>",
        "conflictResolution": "<observation>",
        "communication": "<observation>"
    }},
    "strengths": ["<strength1>", "<strength2>", ...],
    "improvements": ["<improvement1>", "<improvement2>", ...],
    "keyPhrases": {{
        "effective": [
            {{"text": "<phrase>", "note": "<why effective>"}}
        ],
        "toImprove": [
            {{"text": "<phrase>", "note": "<how to improve>"}}
        ]
    }},
    "recommendation": "<text>"
}}

Transcript:
{transcript_text}"""
        
        messages = [
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.generate_chat_response(
            messages=messages,
            system_prompt=system_prompt,
            temperature=0.5,
            max_tokens=2000
        )
        
        # Parse JSON response (GPT sometimes wraps in markdown)
        import json
        import re
        
        # Try to extract JSON from response
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        # Fallback: return structured response
        return {
            "score": 75,
            "summary": response[:500] if response else "Analysis completed",
            "readiness": "Further evaluation needed",
            "observations": {},
            "strengths": [],
            "improvements": [],
            "keyPhrases": {"effective": [], "toImprove": []},
            "recommendation": "Review transcript manually"
        }
    
    async def generate_session_question_structured(
        self, 
        context: GPTContextRequest
    ) -> GPTResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Args:
            context: GPTContextRequest —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–Ω—Ç–µ—Ä–≤—å—é
            
        Returns:
            GPTResponse —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º
        """
        return await self.generate_session_question_with_json_mode(context)
    
    async def generate_session_question_with_json_mode(
        self,
        context: GPTContextRequest
    ) -> GPTResponse:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç JSON mode –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ JSON –æ—Ç–≤–µ—Ç–∞
        
        Args:
            context: GPTContextRequest —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–Ω—Ç–µ—Ä–≤—å—é
            
        Returns:
            GPTResponse —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –æ—Ç GPT
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = self._build_session_system_prompt(context)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å–µ—Å—Å–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è –±–æ–ª—å—à–µ —á–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ)
            is_resume = len(context.conversation_history) > 1
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
            user_prompt = self._build_session_user_prompt(context, is_resume=is_resume)

            # For DeepSeek, add explicit JSON formatting instructions since response_format might not work
            if self.provider == "deepseek":
                user_prompt += "\n\nIMPORTANT: Respond ONLY with valid JSON. Do not include any text before or after the JSON. The response must be parseable JSON."

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # –í—ã–∑–æ–≤ GPT API —Å JSON mode
            print(f"[AI] Calling GPT API with model {self.model_gpt}")
            print(f"[AI] System prompt length: {len(system_prompt)}")
            print(f"[AI] User prompt length: {len(user_prompt)}")
            
            # Prepare API call parameters
            # Optimized for faster responses:
            # - Lower temperature for more deterministic (faster) responses
            # - Reduced max_tokens since interview questions are typically concise
            api_params = {
                "model": self.model_gpt,
                "messages": messages,
                "temperature": 0.5,  # Reduced from 0.7 for faster, more deterministic responses
                "max_tokens": 800,   # Reduced from 1000 - interview questions don't need very long responses
            }

            # Enable streaming for faster response times
            # Streaming provides incremental responses and better perceived performance
            use_streaming = True
            if use_streaming:
                api_params["stream"] = True

            # Enable JSON mode for DeepSeek
            # DeepSeek supports response_format according to their documentation
            api_params["response_format"] = {"type": "json_object"}

            # Handle streaming vs non-streaming responses (retry on connection error)
            response_text = None
            for attempt in range(3):
                try:
                    if use_streaming:
                        print(f"[{self.provider.upper()}] Starting streaming request with model {self.model_gpt} (attempt {attempt + 1}/3)")
                        response_text = await self._handle_streaming_response(api_params)
                    else:
                        response = await self.client.chat.completions.create(**api_params)
                        response_text = response.choices[0].message.content
                    break
                except Exception as req_err:
                    err_str = str(req_err).lower()
                    if ("connection" in err_str or "connect" in err_str) and attempt < 2:
                        print(f"[AI] Connection error (attempt {attempt + 1}/3), retrying in 2s: {req_err}")
                        await asyncio.sleep(2)
                        continue
                    raise
            
            print(f"[{self.provider.upper()}] Response received, length: {len(response_text) if response_text else 0}")
            print(f"[{self.provider.upper()}] Response preview: {response_text[:200] if response_text else 'EMPTY'}...")

            # Handle empty response
            if not response_text or response_text.strip() == "":
                print(f"[{self.provider.upper()}] Empty response received, using fallback")
                # Create a fallback response
                fallback_response = {
                    "question": {
                        "text": "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ –≤–∞—à–µ–º –æ–ø—ã—Ç–µ —Ä–∞–±–æ—Ç—ã –ø–æ –¥–∞–Ω–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.",
                        "type": "main",
                        "isClarifying": False,
                        "isDynamic": False,
                        "parentSessionQuestionAnswerId": None
                    },
                    "metadata": {
                        "needsClarification": False,
                        "answerQuality": "complete",
                        "shouldMoveNext": True,
                        "estimatedTimeRemaining": 25
                    }
                }
                return GPTResponse(**fallback_response)

            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            try:
                response_data = json.loads(response_text)
                print(f"[AI] JSON parsed successfully")
                
                # Process metadata to handle None values and type conversions
                if "metadata" in response_data and response_data["metadata"]:
                    metadata = response_data["metadata"]

                    # Convert estimatedTimeRemaining from float to int if present
                    if "estimatedTimeRemaining" in metadata:
                        estimated_time = metadata["estimatedTimeRemaining"]
                        if isinstance(estimated_time, float):
                            metadata["estimatedTimeRemaining"] = int(estimated_time)
                            print(f"[AI] Converted estimatedTimeRemaining from float to int: {estimated_time} -> {int(estimated_time)}")

                    # Ensure answerQuality is not None (set default if needed)
                    if "answerQuality" in metadata and metadata["answerQuality"] is None:
                        metadata["answerQuality"] = "complete"
                        print(f"[AI] Set default answerQuality: complete")

                    # Ensure other metadata fields are not None
                    if "needsClarification" in metadata and metadata["needsClarification"] is None:
                        metadata["needsClarification"] = False
                    if "shouldMoveNext" in metadata and metadata["shouldMoveNext"] is None:
                        metadata["shouldMoveNext"] = False
                else:
                    # If no metadata provided, add default metadata
                    response_data["metadata"] = {
                        "needsClarification": False,
                        "answerQuality": "complete",
                        "shouldMoveNext": False,
                        "estimatedTimeRemaining": None
                    }
                    print(f"[AI] Added default metadata")
                
                result = GPTResponse(**response_data)
                print(f"[AI] GPTResponse created, question: {result.question.text[:100]}...")
                print(f"[AI] Metadata: {result.metadata}")
                return result
            except json.JSONDecodeError as e:
                print(f"[AI] JSON decode error: {e}")
                print(f"[AI] Response text: {response_text}")
                # –ï—Å–ª–∏ JSON –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –µ–≥–æ
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    response_data = json.loads(json_match.group())
                    return GPTResponse(**response_data)
                else:
                    raise Exception(f"Invalid JSON response from GPT: {response_text}")
                    
        except Exception as e:
            print(f"[AI] Exception in generate_session_question_with_json_mode: {str(e)}")
            import traceback
            traceback.print_exc()
            hint = ""
            if "connection" in str(e).lower() or "connect" in str(e).lower():
                hint = " –ü—Ä–æ–≤–µ—Ä—å—Ç–µ DEEPSEEK_API_KEY –≤ .env, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å https://api.deepseek.com"
            raise Exception(f"Error generating session question: {str(e)}.{hint}")

    async def _handle_streaming_response(self, api_params: dict) -> str:
        """
        Handle streaming response from API and accumulate chunks into complete JSON
        
        Args:
            api_params: API parameters for the request
            
        Returns:
            Complete response text as string
        """
        try:
            print(f"[{self.provider.upper()}] Starting streaming response handling")
            # When stream=True, create() is async ‚Äî await returns the async iterator
            stream = await self.client.chat.completions.create(**api_params)
            
            accumulated_text = ""
            chunk_count = 0
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        content = delta.content
                        accumulated_text += content
                        chunk_count += 1
                        
                        # Log progress every 10 chunks
                        if chunk_count % 10 == 0:
                            print(f"[{self.provider.upper()}] Received {chunk_count} chunks, {len(accumulated_text)} chars so far...")
            
            print(f"[{self.provider.upper()}] Streaming complete: {chunk_count} chunks, {len(accumulated_text)} total chars")
            return accumulated_text.strip()
            
        except Exception as e:
            print(f"[{self.provider.upper()}] Error in streaming response: {e}")
            import traceback
            traceback.print_exc()
            # Fallback: try non-streaming request
            print(f"[{self.provider.upper()}] Falling back to non-streaming request")
            api_params_no_stream = api_params.copy()
            api_params_no_stream.pop("stream", None)
            response = await self.client.chat.completions.create(**api_params_no_stream)
            return response.choices[0].message.content if response.choices[0].message.content else ""
    
    def _build_session_system_prompt(self, context: GPTContextRequest) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–∫—Ä–∏–Ω–∏–Ω–≥-—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è"""
        interview = context.interview
        personality_descriptions = {
            "friendly": "–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º, —Ç–µ–ø–ª—ã–º –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º",
            "professional": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º, –≤–µ–∂–ª–∏–≤—ã–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º",
            "motivating": "–º–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–º, —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–∏—á–Ω—ã–º –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º"
        }
        
        personality_desc = personality_descriptions.get(interview.personality, "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º")
        
        prompt = f"""–¢—ã AI-–∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä, –ø—Ä–æ–≤–æ–¥—è—â–∏–π —Å–∫—Ä–∏–Ω–∏–Ω–≥-—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–º–ø–∞–Ω–∏—é {interview.company or "–∫–æ–º–ø–∞–Ω–∏—é"} –Ω–∞ –ø–æ–∑–∏—Ü–∏—é {interview.position}.

–í–ê–ñ–ù–û: –≠—Ç–æ —Å–∫—Ä–∏–Ω–∏–Ω–≥-—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ (–ø–µ—Ä–≤–∏—á–Ω—ã–π –æ—Ç–±–æ—Ä), –∞ –Ω–µ –ø–æ–ª–Ω–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é. –¶–µ–ª—å - –±—ã—Å—Ç—Ä–æ –æ—Ü–µ–Ω–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏, –º–æ—Ç–∏–≤–∞—Ü–∏—é –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞.

–ü–†–ê–í–ò–õ–ê –ü–û–í–ï–î–ï–ù–ò–Ø:

0. –ü–†–ò–í–ï–¢–°–¢–í–ò–ï –ò –ü–†–û–í–ï–†–ö–ê –ì–û–¢–û–í–ù–û–°–¢–ò (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Å—Å–∏–∏):
   - –ü—Ä–∏ –ø–µ—Ä–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ –ø–æ–∑–¥–æ—Ä–æ–≤–∞–π—Å—è –∏ —Å–ø—Ä–æ—Å–∏ –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏: "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –ø—Ä–æ–≤–æ–∂—É —Å–∫—Ä–∏–Ω–∏–Ω–≥-—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–º–ø–∞–Ω–∏—é {interview.company or "–∫–æ–º–ø–∞–Ω–∏—é"} –Ω–∞ –ø–æ–∑–∏—Ü–∏—é {interview.position}. –ì–æ—Ç–æ–≤—ã –ª–∏ –≤—ã –Ω–∞—á–∞—Ç—å?"
   - –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç –æ—Ç–≤–µ—á–∞–µ—Ç, —á—Ç–æ –≥–æ—Ç–æ–≤ (–¥–∞, –∫–æ–Ω–µ—á–Ω–æ, –≥–æ—Ç–æ–≤ –∏ —Ç.–¥.) - –∑–∞–¥–∞–π –ø–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞
   - –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç –æ—Ç–≤–µ—á–∞–µ—Ç, —á—Ç–æ –Ω–µ –≥–æ—Ç–æ–≤ (–Ω–µ—Ç, –ø–æ–¥–æ–∂–¥–∏—Ç–µ, –Ω–µ –≥–æ—Ç–æ–≤ –∏ —Ç.–¥.) - –≤–µ–∂–ª–∏–≤–æ –ø–æ–ø—Ä–æ—Å–∏ –ø–æ–¥–∞—Ç—å —Å–∏–≥–Ω–∞–ª –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏: "–•–æ—Ä–æ—à–æ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π—Ç–µ –∑–Ω–∞—Ç—å, –∫–æ–≥–¥–∞ –±—É–¥–µ—Ç–µ –≥–æ—Ç–æ–≤—ã –Ω–∞—á–∞—Ç—å"
   - –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ—è—Å–Ω—ã–π - —É—Ç–æ—á–Ω–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å

1. –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ï –í–û–ü–†–û–°–´:"""
        
        # –ú–µ–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç allow_dynamic_questions
        if context.allow_dynamic_questions:
            prompt += """
   - –¢—ã –º–æ–∂–µ—à—å –∑–∞–¥–∞–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –µ—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º
   - –ù–û –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –≤—Å–µ–≥–¥–∞ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ
   - –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —É–∫–∞–∑–∞–Ω —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞–π –µ–≥–æ, –∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∑–∞–¥–∞–≤–∞–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
   - –ï—Å–ª–∏ —Ç—ã —Ä–µ—à–∏–ª –∑–∞–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–µ—Ä–µ–¥ –≤–æ–ø—Ä–æ—Å–æ–º –∏–∑ —à–∞–±–ª–æ–Ω–∞, —É–∫–∞–∂–∏ –≤ –æ—Ç–≤–µ—Ç–µ isDynamic = true"""
        else:
            prompt += """
   - –¢—ã –º–æ–∂–µ—à—å –∑–∞–¥–∞–≤–∞—Ç—å –¢–û–õ–¨–ö–û –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –∏ –∏—Ö —É—Ç–æ—á–Ω—è—é—â–∏–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã
   - –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Å–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã
   - –ù–µ –∑–∞–¥–∞–≤–∞–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã (isDynamic –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å false)"""
        
        prompt += f"""

2. –û–°–¢–ê–í–®–ï–ï–°–Ø –í–†–ï–ú–Ø:
   - –ï—Å–ª–∏ –≤—Ä–µ–º–µ–Ω–∏ < 5 –º–∏–Ω—É—Ç: –ù–ï –∑–∞–¥–∞–≤–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã
   - –ï—Å–ª–∏ –≤—Ä–µ–º–µ–Ω–∏ –º–Ω–æ–≥–æ: –º–æ–∂–µ—à—å –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
   - –ï—Å–ª–∏ –≤—Ä–µ–º—è –∏—Å—Ç–µ–∫–ª–æ: –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –∑–∞–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ

3. –£–¢–û–ß–ù–Ø–Æ–©–ò–ï –í–û–ü–†–û–°–´:
   - –ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–Ω
   - –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–æ–∏—Ç —É—Ç–æ—á–Ω–∏—Ç—å - —É—Ç–æ—á–Ω–∏ –µ–≥–æ
   - –ó–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–¥–Ω–æ–º—É

4. –ü–ï–†–ï–°–ü–†–û–°:
   - –ï—Å–ª–∏ –≤—Ä–µ–º–µ–Ω–∏ –º–Ω–æ–≥–æ –∏ –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–∞–ª –Ω–µ–≤–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç - –º–æ–∂–µ—à—å –ø–µ—Ä–µ—Å–ø—Ä–æ—Å–∏—Ç—å –≤–æ–ø—Ä–æ—Å, —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–≤ –µ–≥–æ –ø–æ-–¥—Ä—É–≥–æ–º—É

5. –ó–ê–î–ê–í–ê–ô –í–û–ü–†–û–°–´ –ü–û –û–î–ù–û–ú–£ - –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∑–∞–¥–∞–≤–∞–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å—Ä–∞–∑—É

6. –ù–ï –ü–û–í–¢–û–†–Ø–ô –í–û–ü–†–û–°–´ - –°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:
   - –ù–ò–ö–û–ì–î–ê –Ω–µ –∑–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –±—ã–ª –∑–∞–¥–∞–Ω –≤ —ç—Ç–æ–º –∏–Ω—Ç–µ—Ä–≤—å—é
   - –í—Å–µ –∑–∞–¥–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã –≤ —Ä–∞–∑–¥–µ–ª–µ "–£–ñ–ï –ó–ê–î–ê–ù–ù–´–ï –í–û–ü–†–û–°–´"
   - –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –±—ã–ª –∑–∞–¥–∞–Ω - –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É –∏–∑ —à–∞–±–ª–æ–Ω–∞
   - –ï—Å–ª–∏ –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –∑–∞–¥–∞–Ω—ã - –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–ª–∏ –∑–∞–≤–µ—Ä—à–∏ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ customer_simulation, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        if interview.customer_simulation and interview.customer_simulation.enabled:
            if getattr(context, "simulation_done", False):
                prompt += """

6. –°–ò–ú–£–õ–Ø–¶–ò–Ø –£–ñ–ï –ü–†–û–í–ï–î–ï–ù–ê ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û:
   - –ö–∞–Ω–¥–∏–¥–∞—Ç —É–∂–µ –æ—Ç–≤–µ—Ç–∏–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏. –°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.
   - –ù–ï –∑–∞–¥–∞–≤–∞–π –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—é. –ù–ï –ø–æ–≤—Ç–æ—Ä—è–π –≤–æ–ø—Ä–æ—Å –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏.
   - –ó–∞–≤–µ—Ä—à–∏ –∏–Ω—Ç–µ—Ä–≤—å—é: –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ –∫—Ä–∞—Ç–∫–æ –ø–æ–¥–≤–µ–¥–∏ –∏—Ç–æ–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ç–≤–µ—Ç—ã. –ù–∞ —ç—Ç–æ–º –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ.¬ª)."""
            else:
                prompt += f"""

6. –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ô –†–ê–ë–û–ß–ï–ô –°–ò–¢–£–ê–¶–ò–ò (customer_simulation):
   - –í –∫–æ–Ω—Ü–µ –∏–Ω—Ç–µ—Ä–≤—å—é (–∫–æ–≥–¥–∞ –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∑–∞–¥–∞–Ω—ã –∏–ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å < 5 –º–∏–Ω—É—Ç) –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–∏–º—É–ª—è—Ü–∏—é
   - –í —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏ –∑–∞–¥–∞–π –Ω–µ –±–æ–ª–µ–µ 1‚Äì2 –≤–æ–ø—Ä–æ—Å–æ–≤; –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å—Ä–∞–∑—É –∑–∞–≤–µ—Ä—à–∞–π —Å–∏–º—É–ª—è—Ü–∏—é –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –∏–Ω—Ç–µ—Ä–≤—å—é
   - –ü–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –≤–æ–ø—Ä–æ—Å–æ–º –≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∏–∑–Ω–µ—Å–∏ –∫–æ—Ä–æ—Ç–∫—É—é –≤–≤–æ–¥–Ω—É—é —Ñ—Ä–∞–∑—É, –Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–î–∞–≤–∞–π—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º —Å–∏—Ç—É–∞—Ü–∏—é¬ª –∏–ª–∏ ¬´–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ‚Ä¶¬ª
   - –¢—ã –¥–æ–ª–∂–µ–Ω —Å—ã–≥—Ä–∞—Ç—å —Ä–æ–ª—å –∫–ª–∏–µ–Ω—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ü–µ–Ω–∞—Ä–∏—é:
     * –†–æ–ª—å –∫–ª–∏–µ–Ω—Ç–∞: {interview.customer_simulation.role or "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"}
     * –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è: {interview.customer_simulation.scenario or "–Ω–µ —É–∫–∞–∑–∞–Ω"}
   - –í–µ–¥–∏ —Å–µ–±—è –∫–∞–∫ —É–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–¥–æ–≤–æ–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç, –≥–æ—Å—Ç—å, –∑–∞–∫–∞–∑—á–∏–∫)
   - –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –≤—ã—Å–∫–∞–∑—ã–≤–∞–π –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏ –æ—Ç –ª–∏—Ü–∞ —ç—Ç–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
   - –û—Ü–µ–Ω–∏–≤–∞–π —Ä–µ–∞–∫—Ü–∏—é –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –Ω–∞ —Å—Ç—Ä–µ—Å—Å–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é
   - –°–∏–º—É–ª—è—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–Ω–æ–º—É —Å—Ü–µ–Ω–∞—Ä–∏—é
   - –ü–æ—Å–ª–µ —Å–∏–º—É–ª—è—Ü–∏–∏ –º–æ–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é"""
        
        prompt += f"""

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –ü—Ä–æ–≤–æ–¥–∏ –∏–Ω—Ç–µ—Ä–≤—å—é {personality_desc} –æ–±—Ä–∞–∑–æ–º
- –ó–∞–¥–∞–≤–∞–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–æ–∑–∏—Ü–∏–∏
- –°–ª—É—à–∞–π –∞–∫—Ç–∏–≤–Ω–æ –∏ –∑–∞–¥–∞–≤–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
- –î–µ—Ä–∂–∏ –æ—Ç–≤–µ—Ç—ã –∫—Ä–∞—Ç–∫–∏–º–∏ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏
- –í–µ–¥–∏ –±–µ—Å–µ–¥—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ"""
        
        if context.interview.instructions:
            prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: {context.interview.instructions}\n"
        
        prompt += """

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: –¢—ã –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å:
{
  "question": {
    "text": "—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞",
    "type": "main" | "clarifying" | "dynamic",
    "isClarifying": true/false,
    "isDynamic": true/false,
    "parentSessionQuestionAnswerId": "uuid –∏–ª–∏ null"
  },
  "metadata": {
    "needsClarification": true/false,
    "answerQuality": "complete" | "partial" | "insufficient",
    "shouldMoveNext": true/false,
    "estimatedTimeRemaining": —á–∏—Å–ª–æ (–º–∏–Ω—É—Ç—ã)
  },
  "analysis": {
    "keyPoints": ["–∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 1", "–∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 2"],
    "suggestedFollowUps": ["–≤–æ–ø—Ä–æ—Å 1", "–≤–æ–ø—Ä–æ—Å 2"]
  }
}
"""
        
        return prompt
    
    def _build_session_user_prompt(self, context: GPTContextRequest, is_resume: bool = False) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        interview = context.interview
        remaining_minutes = context.remaining_time.minutes
        remaining_seconds = context.remaining_time.seconds
        
        prompt = f"""–ö–û–ù–¢–ï–ö–°–¢ –°–ö–†–ò–ù–ò–ù–ì-–°–û–ë–ï–°–ï–î–û–í–ê–ù–ò–Ø:"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å–µ—Å—Å–∏–∏
        if is_resume:
            prompt += f"""

‚ö†Ô∏è –í–ê–ñ–ù–û: –°–µ—Å—Å–∏—è –±—ã–ª–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ù–∏–∂–µ –ø–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞."""
        
        prompt += f"""

–ü–æ–∑–∏—Ü–∏—è: {interview.position}
–ö–æ–º–ø–∞–Ω–∏—è: {interview.company or "–ù–µ —É–∫–∞–∑–∞–Ω–∞"}
–û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: {remaining_minutes} –º–∏–Ω—É—Ç {remaining_seconds} —Å–µ–∫—É–Ω–¥"""
        
        # –í–ª–∏—è–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
        if remaining_minutes < 5:
            prompt += "\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—Ä–µ–º–µ–Ω–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–∞–ª–æ! –ù–ï –∑–∞–¥–∞–≤–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã."
        elif remaining_minutes < 10:
            prompt += "\n‚ö†Ô∏è –í—Ä–µ–º–µ–Ω–∏ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–º–Ω–æ–≥–æ. –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö."
        else:
            prompt += "\n‚úÖ –í—Ä–µ–º–µ–Ω–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ. –ú–æ–∂–µ—à—å –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è."
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º (–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ–º)
        is_first_message = len(context.conversation_history) == 0
        
        if is_first_message:
            # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ - –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
            prompt += f"""

üéØ –ò–ù–°–¢–†–£–ö–¶–ò–Ø –î–õ–Ø –ü–ï–†–í–û–ì–û –°–û–û–ë–©–ï–ù–ò–Ø (–ü–†–ò–í–ï–¢–°–¢–í–ò–ï):
–¢—ã –¥–æ–ª–∂–µ–Ω –ø–æ–∑–¥–æ—Ä–æ–≤–∞—Ç—å—Å—è –∏ —Å–ø—Ä–æ—Å–∏—Ç—å –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –Ω–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é.
–§–æ—Ä–º–∞—Ç: "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –ø—Ä–æ–≤–æ–∂—É —Å–∫—Ä–∏–Ω–∏–Ω–≥-—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–º–ø–∞–Ω–∏—é {interview.company or "–∫–æ–º–ø–∞–Ω–∏—é"} –Ω–∞ –ø–æ–∑–∏—Ü–∏—é {interview.position}. –ì–æ—Ç–æ–≤—ã –ª–∏ –≤—ã –Ω–∞—á–∞—Ç—å?"

–í–ê–ñ–ù–û: –≠—Ç–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –ù–ï –∑–∞–¥–∞–≤–∞–π –ø–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞ —Å–µ–π—á–∞—Å. –°–Ω–∞—á–∞–ª–∞ –¥–æ–∂–¥–∏—Å—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞."""
        else:
            # –°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞ (–í–°–ï–ì–î–ê —É–∫–∞–∑—ã–≤–∞—Ç—å –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)
            if context.current_interview_question:
                current_q = context.current_interview_question
                prompt += f"""

–°–õ–ï–î–£–Æ–©–ò–ô –í–û–ü–†–û–° –ò–ó –®–ê–ë–õ–û–ù–ê:
- –¢–µ–∫—Å—Ç: {current_q.text}
- –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä: {current_q.order_index + 1}"""
                
                if current_q.clarifying_questions:
                    prompt += f"\n- –£—Ç–æ—á–Ω—è—é—â–∏–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞:\n"
                    for i, clar_q in enumerate(current_q.clarifying_questions, 1):
                        prompt += f"  {i}. {clar_q}\n"
                
                # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å
                if context.allow_dynamic_questions:
                    prompt += f"""
\n–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞: "{current_q.text}"
–¢—ã –º–æ–∂–µ—à—å:
1. –ó–∞–¥–∞—Ç—å —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞ (isDynamic = false)
2. –ò–õ–ò –µ—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω—ã–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞—Ç—å —Å–≤–æ–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å (isDynamic = true), –∞ –∑–∞—Ç–µ–º –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞
–ù–û: –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –≤—Å–µ–≥–¥–∞ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ!"""
                else:
                    prompt += f"""
\n–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ó–∞–¥–∞–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–∑ —à–∞–±–ª–æ–Ω–∞: "{current_q.text}"
–¢—ã –º–æ–∂–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–Ω.
–ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Å–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã (isDynamic –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å false)."""
            else:
                # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ —à–∞–±–ª–æ–Ω–∞ –±–æ–ª—å—à–µ –Ω–µ—Ç
                prompt += "\n\n‚ö†Ô∏è –í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –∑–∞–¥–∞–Ω—ã."
                if context.allow_dynamic_questions:
                    prompt += " –¢—ã –º–æ–∂–µ—à—å –∑–∞–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –µ—Å–ª–∏ —ç—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏."
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
        if context.conversation_history:
            prompt += "\n\n–ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê:"
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5-10 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            recent_history = context.conversation_history[-10:] if len(context.conversation_history) > 10 else context.conversation_history
            for msg in recent_history:
                role_label = "AI" if msg.role == "ai" else "–ö–∞–Ω–¥–∏–¥–∞—Ç"
                prompt += f"\n{role_label}: {msg.message}"
        
        # –ò—Å—Ç–æ—Ä–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
        if context.session_history:
            prompt += "\n\n–ò–°–¢–û–†–ò–Ø –í–û–ü–†–û–°–û–í –ò –û–¢–í–ï–¢–û–í:"
            for i, qa in enumerate(context.session_history[-5:], 1):  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                prompt += f"\n{i}. [{qa.question_type.upper()}] {qa.question_text}"
                prompt += f"\n   –û—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: {qa.answer_text}"

            # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–∂–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            all_asked_questions = [qa.question_text for qa in context.session_history]
            if all_asked_questions:
                prompt += f"\n\n–£–ñ–ï –ó–ê–î–ê–ù–ù–´–ï –í–û–ü–†–û–°–´ (–ù–ï–õ–¨–ó–Ø –ü–û–í–¢–û–†–Ø–¢–¨):"
                for i, question in enumerate(all_asked_questions, 1):
                    prompt += f"\n{i}. {question}"
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if context.user_response:
            prompt += f"\n\n–ü–û–°–õ–ï–î–ù–ò–ô –û–¢–í–ï–¢ –ö–ê–ù–î–ò–î–ê–¢–ê:\n{context.user_response.text}"
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        progress = context.question_progress
        prompt += f"""

–ü–†–û–ì–†–ï–°–°:
- –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å: {progress.current_question_index + 1} –∏–∑ {progress.total_questions}
- –û—Ç–≤–µ—á–µ–Ω–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {progress.answered_questions}"""
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ customer_simulation
        if interview.customer_simulation and interview.customer_simulation.enabled:
            simulation_done = getattr(context, "simulation_done", False)
            if simulation_done:
                prompt += """

üé≠ –°–ò–ú–£–õ–Ø–¶–ò–Ø –£–ñ–ï –ü–†–û–í–ï–î–ï–ù–ê:
–ö–∞–Ω–¥–∏–¥–∞—Ç —É–∂–µ –æ—Ç–≤–µ—Ç–∏–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏. –ù–ï –∑–∞–¥–∞–≤–∞–π –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—é. –ù–ï –ø–æ–≤—Ç–æ—Ä—è–π –≤–æ–ø—Ä–æ—Å –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏.
–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ó–∞–≤–µ—Ä—à–∏ –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ –∫—Ä–∞—Ç–∫–æ –ø–æ–¥–≤–µ–¥–∏ –∏—Ç–æ–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ç–≤–µ—Ç—ã. –ù–∞ —ç—Ç–æ–º –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ.¬ª)."""
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∫ –∫–æ–Ω—Ü—É
                all_questions_asked = progress.current_question_index >= progress.total_questions
                time_low = remaining_minutes < 5
                
                if all_questions_asked or time_low:
                    prompt += f"""

üé≠ –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ô –†–ê–ë–û–ß–ï–ô –°–ò–¢–£–ê–¶–ò–ò:
–ò–Ω—Ç–µ—Ä–≤—å—é –ø–æ–¥—Ö–æ–¥–∏—Ç –∫ –∫–æ–Ω—Ü—É. –¢—ã –º–æ–∂–µ—à—å –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–∏–º—É–ª—è—Ü–∏—é —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—á–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏.
- –†–æ–ª—å –∫–ª–∏–µ–Ω—Ç–∞: {interview.customer_simulation.role or "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"}
- –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è: {interview.customer_simulation.scenario or "–Ω–µ —É–∫–∞–∑–∞–Ω"}

–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –°—ã–≥—Ä–∞–π —Ä–æ–ª—å —ç—Ç–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ø—Ä–æ–≤–µ–¥–∏ —Å–∏–º—É–ª—è—Ü–∏—é. –í–µ–¥–∏ —Å–µ–±—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ —Å—Ü–µ–Ω–∞—Ä–∏—é.
–ó–∞–¥–∞–π –≤ —ç—Ç–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏ –Ω–µ –±–æ–ª–µ–µ 1‚Äì2 –≤–æ–ø—Ä–æ—Å–æ–≤. –ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∑–∞–≤–µ—Ä—à–∏ —Å–∏–º—É–ª—è—Ü–∏—é, –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–π —Ä–∞–∑—ã–≥—Ä—ã–≤–∞—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–π.
–ù–∞—á–Ω–∏ —Å –≤–≤–æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–î–∞–≤–∞–π—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º —Å–∏—Ç—É–∞—Ü–∏—é¬ª –∏–ª–∏ ¬´–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ‚Ä¶¬ª.
–û—Ü–µ–Ω–∏–≤–∞–π —Ä–µ–∞–∫—Ü–∏—é –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –Ω–∞ —Å—Ç—Ä–µ—Å—Å–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é. –ü–æ—Å–ª–µ —Å–∏–º—É–ª—è—Ü–∏–∏ –º–æ–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é."""
        
        prompt += "\n\n–ó–∞–¥–∞–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏–∑ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞."
        
        return prompt

